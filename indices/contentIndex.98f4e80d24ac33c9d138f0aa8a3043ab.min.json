{"/":{"title":"_index","content":"\nTesting\n","lastmodified":"2023-06-17T01:54:51.062605376Z","tags":[]},"/notes/callouts":{"title":"Callouts","content":"\n## Callout support\n\nQuartz supports the same Admonition-callout syntax as Obsidian.\n\nThis includes\n- 12 Distinct callout types (each with several aliases)\n- Collapsable callouts\n\nSee [documentation on supported types and syntax here](https://help.obsidian.md/Editing+and+formatting/Callouts).\n\n## Showcase\n\n\u003e [!EXAMPLE] Examples\n\u003e\n\u003e Aliases: example\n\n\u003e [!note] Notes\n\u003e\n\u003e Aliases: note\n\n\u003e [!abstract] Summaries \n\u003e\n\u003e Aliases: abstract, summary, tldr\n\n\u003e [!info] Info \n\u003e\n\u003e Aliases: info, todo\n\n\u003e [!tip] Hint \n\u003e\n\u003e Aliases: tip, hint, important\n\n\u003e [!success] Success \n\u003e\n\u003e Aliases: success, check, done\n\n\u003e [!question] Question \n\u003e\n\u003e Aliases: question, help, faq\n\n\u003e [!warning] Warning \n\u003e\n\u003e Aliases: warning, caution, attention\n\n\u003e [!failure] Failure \n\u003e\n\u003e Aliases: failure, fail, missing\n\n\u003e [!danger] Error\n\u003e\n\u003e Aliases: danger, error\n\n\u003e [!bug] Bug\n\u003e\n\u003e Aliases: bug\n\n\u003e [!quote] Quote\n\u003e\n\u003e Aliases: quote, cite\n","lastmodified":"2023-06-17T01:54:51.062605376Z","tags":[]},"/notes/spark_architecture_overview":{"title":"Spark - an overview of architecture","content":"\n## Overview \n\nSpark is a popular open-source framework that provides several tools for analytics and processing of data in parallel fashion. While MapReduce-based systems often involve disk-expensive operations, computation in Spark avoids this by keeping jobs in memory. Let's examine the following components of Spark:\n- Resilient Distributed Dataset\n- Directed acyclic graph\n- Resilient Distributed Dataset (RDD)\n\n## Resilient Distributed Dataset\n\nRDD is a Spark data structure representing a set of data that get partitioned and distributed among cluster nodes for parallel processing. There are 2 types of operations that can be performed on RDD: transformation and action\n\na. Transformation \n\nOne of the characteristic of RDD is that it is immutable. If we would like to modify the data, we have to give Spark instructions on how to do it. These instructions are what called transformation. There are 2 types of transformation: narrow and wide. The former type doesn’t require data to be shuffled and reorganized between partitions while the latter does. Specifically, shuffling involves having input partitions contributing to other output partitions. Some of the basic RDD transformation operations that one would most likely to come across include map(), filter(),flatMap(), union(), etc. \n\nb. Action \n\nActions are any functions that triggers the plan in transformation to be executed to return a result. To put this another way, when we apply transformation to RDD, those operations won’t get executed immediately but are postponed until an action is performed. This is because Spark uses lazy evaluation, a mechanism which allows Spark to optimize logical plan and convert it to a physical plan for execution. Examples of action operations include collect(), take(N), first() and count(). \n\n## Directed acyclic graph (DAG)\n\nDAG can be think of as an object representing a logical flow of operations. It is used in the scheduling layer of Spark to convert logical execution plan to physical one. Recall that when we apply transformation onto RDD, they are lazily evaluated. This means that Spark doesn’t execute the operations right away but building up a logical plan of operations. In order to orchestrate such flow of operations, Spark uses DAG to define and schedule tasks.\n\n\n## Bring it all together: How does a job get processed in Spark? \n\nIn Spark, a job is a unit of parallel computation and is transformed into a DAG object to represent Spark’s execution plan. A job may consist of multiple stages, each of which is a node/vertex on the DAG. At the lowest abstraction level, a stage is further broken down into tasks, which are units of execution that get federated to executors. Each task maps to a single core and works on a single data partition.  Therefore, suppose we have an executor that has N cores and is capable of handling 1 or more task for each core, then we would have 1*N or more tasks working on N or more partitions in parallel. ","lastmodified":"2023-06-17T01:54:51.066605403Z","tags":[]},"/notes/spark_joins":{"title":"Making sense of Shuffle hash join and Broadcast join in Spark","content":"\n## Shuffle hash join \n\nShuffle hash join is a join strategy where repartitioning is based on join key. This is considered as a wide transformation since shuffling of data between partitions is required, hence the word \"shuffle\" in its name. \n\nSuppose we have partitions of dataset A and dataset B residing in executor nodes. Typically, there are 2 main steps involved in shuffle hash join.  \n\n![Shuffle hash join](notes/images/spark-shufflehash-join.jpg)\n\nFirst, based on the join key, Spark will shuffle data so that each partition contains records with the same key for both dataset A and dataset B. Once we have all the partitions together, Spark will take relation of smaller size (in this case, let's assume that it is dataset A) to create a hash table and perform hash join within the partition. \n\n## Broadcast join \n\n![Broadcast join](notes/images/spark-broadcast-join.jpg)\n\nBroadcast join is another join strategy where a copy of the smaller dataset (such as A) will be broadcasted to all executors so that join operation can be done locally. This eliminates the cost incurred with shuffling data between partitions. \n\nSpecifically, partitions of dataset A in executor nodes will be sent to the driver node. Dataset A is then broadcasted from driver to executor nodes. Now that each executor node has a copy of dataset A, join operation can be done locally within the node. ","lastmodified":"2023-06-17T01:54:51.066605403Z","tags":[]}}